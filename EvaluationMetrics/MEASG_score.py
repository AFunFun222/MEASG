# !/usr/bin/env python
# !-*-coding:utf-8 -*-
import pickle
import sys
sys.path.append("./EvaluationMetrics")
from meteor.meteor import Meteor
from rouge.rouge import Rouge
from cider.cider import Cider
import nltk

def metetor_rouge_cider(refs, preds):
    refs_dict = {}
    preds_dict = {}
    for i in range(len(preds)):
        preds_dict[i] = [" ".join(preds[i])]
        refs_dict[i] = [" ".join(refs[i])]
    score_Rouge, scores_Rouge = Rouge().compute_score(refs_dict, preds_dict)
    score_Cider, scores_Cider = Cider().compute_score(refs_dict, preds_dict)
    score_Meteor, scores_Meteor = Meteor().compute_score(refs_dict, preds_dict)

    return score_Rouge, score_Cider, score_Meteor

def sentence_bleu_score(reference, candidate) -> float:
    """
    calculate the sentence level bleu score, 4-gram with weights(0.25, 0.25, 0.25, 0.25)
    :param reference: tokens of reference sentence
    :param candidate: tokens of sentence generated by model
    :return: sentence level bleu score
    """
    smoothing_function = nltk.translate.bleu_score.SmoothingFunction()
    return nltk.translate.bleu_score.sentence_bleu(references=[reference],
                                                   hypothesis=candidate,
                                                   smoothing_function=smoothing_function.method5)


def s_bleu_compute(references, candidates) -> (float, float):
    """
    measures the top sentence model generated
    :param references: batch of references
    :param candidates: batch of sentences model generated
    :return: total sentence level bleu score
    """
    assert len(references) == len(candidates)
    batch_size = len(references)
    total_s_bleu = 0
    for reference, candidate in zip(references, candidates):
        # sentence level bleu score
        sentence_bleu = sentence_bleu_score(reference, candidate)
        total_s_bleu += sentence_bleu

    return total_s_bleu / batch_size


if __name__ == '__main__':
    test_code = pickle.load(open('./data/MEASG_test_summary.pkl', 'rb'))
    total_references = test_code['references']
    total_candidates = test_code['candidates']
    s_blue_score = s_bleu_compute(references=total_references, candidates=total_candidates)
    score_Rouge, score_Cider, score_Meteor = metetor_rouge_cider(total_references, total_candidates)
    print('s_bleu=%.6f | Rouge= %.6f | Cider = %.6f | Meteor = %.6f | ' % (s_blue_score * 100, score_Rouge * 100,
                                                                           score_Cider, score_Meteor * 100))




